{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed342340",
   "metadata": {},
   "source": [
    "<img src=\"../../../figs/holberton_logo.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7e04d",
   "metadata": {},
   "source": [
    "## 0. RNN Cell\n",
    "\n",
    "<img src=\"figs/RNN.png\" alt=\"logo\" width=\"350\"/>\n",
    "\n",
    "### Overall idea\n",
    "\n",
    "Our goal is to implement `RNNCell` class, representing a single cell in a simple Recurrent Neural Network (RNN). The class is designed to handle the forward propagation of one time step, managing the transition of hidden states and the generation of outputs based on input data.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "#### 1. Initialization of Parameters (Weights and Biases)\n",
    "\n",
    "- Initialize weights for hidden state and input (`Wh`).\n",
    "- Initialize weights for output (`Wy`).\n",
    "- Initialize biases for hidden state (`bh`) and output (`by`).\n",
    "\n",
    "#### 2. Forward Propagation:\n",
    "\n",
    "##### Concatenate Inputs:\n",
    "- Combine previous hidden state (`h_prev`) and current input (`x_t`).\n",
    "\n",
    "##### Compute Hidden State:\n",
    "- Apply `tanh` activation to the linear combination of concatenated inputs and weights (`Wh`), plus bias (`bh`).\n",
    "\n",
    "<img src=\"figs/tanh.png\" alt=\"logo\" width=\"300\"/>\n",
    "\n",
    "\n",
    "##### Generate Output:\n",
    "- Apply softmax activation to the linear combination of the new hidden state and output weights (`Wy`), plus bias (`by`).\n",
    "\n",
    "<img src=\"figs/softmax.png\" alt=\"logo\" width=\"150\"/>\n",
    "\n",
    "\n",
    "#### 2. Return Values:\n",
    "- Provide the next hidden state (`h_next`) and the current output (`output_t`).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e776574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"RNN Cell Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNNCell:\n",
    "    \"\"\"\n",
    "        Represents a cell of a simple RNN:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "        \"\"\"\n",
    "            Key concept: an RNN cell uses both the current input \n",
    "            and the previous hidden state to determine the next hidden state\n",
    "            \n",
    "            The concatenation of the input data and hidden state allows the cell \n",
    "            to process the combined information together\n",
    "        \"\"\"\n",
    "        self.Wh = np.random.normal(size=(i+h, h)) # weight matrix for the concatenated input and hidden state\n",
    "        self.Wy = np.random.normal(size=(h, o))   # weight matrix for the output\n",
    "        self.bh = np.zeros((1, h))                # bias for the hidden state, initialized to zeros\n",
    "        self.by = np.zeros((1, o))                # bias for the output, initialized to zeros\n",
    "\n",
    "    def forward(self, h_prev, x_t):\n",
    "        \"\"\"\n",
    "            Performs forward propagation for one time step\n",
    "        \"\"\"\n",
    "\n",
    "        def softmax(x):\n",
    "            \"\"\"computes the softmax activation, used to convert the output logits into probabilities\"\"\"\n",
    "            return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "        # Concatenation of Hidden State and Input\n",
    "        h_x = np.concatenate((h_prev, x_t), axis=1)\n",
    "\n",
    "        # The next hidden state is calculated using the tanh activation function \n",
    "        # applied to the linear combination of h_x and the weights Wh, plus the bias bh.\n",
    "        h_next = np.tanh(np.dot(h_x, self.Wh) + self.bh)\n",
    "        \n",
    "        # The output is computed by applying the softmax function to the \n",
    "        # linear combination of the new hidden state h_next and the weights Wy, plus the bias by\n",
    "        output_t = softmax(np.dot(h_next, self.Wy) + self.by)\n",
    "\n",
    "        # return the next hidden state and the output of the cell for the current time step\n",
    "        return h_next, output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87fe2a",
   "metadata": {},
   "source": [
    "### 0-Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac1b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh: [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721 -0.10321885  0.4105985   0.14404357  1.45427351\n",
      "   0.76103773  0.12167502  0.44386323]\n",
      " [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574 -2.55298982\n",
      "   0.6536186   0.8644362  -0.74216502  2.26975462 -1.45436567  0.04575852\n",
      "  -0.18718385  1.53277921  1.46935877]\n",
      " [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215  0.15634897\n",
      "   1.23029068  1.20237985 -0.38732682 -0.30230275 -1.04855297 -1.42001794\n",
      "  -1.70627019  1.9507754  -0.50965218]\n",
      " [-0.4380743  -1.25279536  0.77749036 -1.61389785 -0.21274028 -0.89546656\n",
      "   0.3869025  -0.51080514 -1.18063218 -0.02818223  0.42833187  0.06651722\n",
      "   0.3024719  -0.63432209 -0.36274117]\n",
      " [-0.67246045 -0.35955316 -0.81314628 -1.7262826   0.17742614 -0.40178094\n",
      "  -1.63019835  0.46278226 -0.90729836  0.0519454   0.72909056  0.12898291\n",
      "   1.13940068 -1.23482582  0.40234164]\n",
      " [-0.68481009 -0.87079715 -0.57884966 -0.31155253  0.05616534 -1.16514984\n",
      "   0.90082649  0.46566244 -1.53624369  1.48825219  1.89588918  1.17877957\n",
      "  -0.17992484 -1.07075262  1.05445173]\n",
      " [-0.40317695  1.22244507  0.20827498  0.97663904  0.3563664   0.70657317\n",
      "   0.01050002  1.78587049  0.12691209  0.40198936  1.8831507  -1.34775906\n",
      "  -1.270485    0.96939671 -1.17312341]\n",
      " [ 1.94362119 -0.41361898 -0.74745481  1.92294203  1.48051479  1.86755896\n",
      "   0.90604466 -0.86122569  1.91006495 -0.26800337  0.8024564   0.94725197\n",
      "  -0.15501009  0.61407937  0.92220667]\n",
      " [ 0.37642553 -1.09940079  0.29823817  1.3263859  -0.69456786 -0.14963454\n",
      "  -0.43515355  1.84926373  0.67229476  0.40746184 -0.76991607  0.53924919\n",
      "  -0.67433266  0.03183056 -0.63584608]\n",
      " [ 0.67643329  0.57659082 -0.20829876  0.39600671 -1.09306151 -1.49125759\n",
      "   0.4393917   0.1666735   0.63503144  2.38314477  0.94447949 -0.91282223\n",
      "   1.11701629 -1.31590741 -0.4615846 ]\n",
      " [-0.06824161  1.71334272 -0.74475482 -0.82643854 -0.09845252 -0.66347829\n",
      "   1.12663592 -1.07993151 -1.14746865 -0.43782004 -0.49803245  1.92953205\n",
      "   0.94942081  0.08755124 -1.22543552]\n",
      " [ 0.84436298 -1.00021535 -1.5447711   1.18802979  0.31694261  0.92085882\n",
      "   0.31872765  0.85683061 -0.65102559 -1.03424284  0.68159452 -0.80340966\n",
      "  -0.68954978 -0.4555325   0.01747916]\n",
      " [-0.35399391 -1.37495129 -0.6436184  -2.22340315  0.62523145 -1.60205766\n",
      "  -1.10438334  0.05216508 -0.739563    1.5430146  -1.29285691  0.26705087\n",
      "  -0.03928282 -1.1680935   0.52327666]\n",
      " [-0.17154633  0.77179055  0.82350415  2.16323595  1.33652795 -0.36918184\n",
      "  -0.23937918  1.0996596   0.65526373  0.64013153 -1.61695604 -0.02432612\n",
      "  -0.73803091  0.2799246  -0.09815039]\n",
      " [ 0.91017891  0.31721822  0.78632796 -0.4664191  -0.94444626 -0.41004969\n",
      "  -0.01702041  0.37915174  2.25930895 -0.04225715 -0.955945   -0.34598178\n",
      "  -0.46359597  0.48148147 -1.54079701]\n",
      " [ 0.06326199  0.15650654  0.23218104 -0.59731607 -0.23792173 -1.42406091\n",
      "  -0.49331988 -0.54286148  0.41605005 -1.15618243  0.7811981   1.49448454\n",
      "  -2.06998503  0.42625873  0.67690804]\n",
      " [-0.63743703 -0.39727181 -0.13288058 -0.29779088 -0.30901297 -1.67600381\n",
      "   1.15233156  1.07961859 -0.81336426 -1.46642433  0.52106488 -0.57578797\n",
      "   0.14195316 -0.31932842  0.69153875]\n",
      " [ 0.69474914 -0.72559738 -1.38336396 -1.5829384   0.61037938 -1.18885926\n",
      "  -0.50681635 -0.59631404 -0.0525673  -1.93627981  0.1887786   0.52389102\n",
      "   0.08842209 -0.31088617  0.09740017]\n",
      " [ 0.39904635 -2.77259276  1.95591231  0.39009332 -0.65240858 -0.39095338\n",
      "   0.49374178 -0.11610394 -2.03068447  2.06449286 -0.11054066  1.02017271\n",
      "  -0.69204985  1.53637705  0.28634369]\n",
      " [ 0.60884383 -1.04525337  1.21114529  0.68981816  1.30184623 -0.62808756\n",
      "  -0.48102712  2.3039167  -1.06001582 -0.1359497   1.13689136  0.09772497\n",
      "   0.58295368 -0.39944903  0.37005589]\n",
      " [-1.30652685  1.65813068 -0.11816405 -0.6801782   0.66638308 -0.46071979\n",
      "  -1.33425847 -1.34671751  0.69377315 -0.15957344 -0.13370156  1.07774381\n",
      "  -1.12682581 -0.73067775 -0.38487981]\n",
      " [ 0.09435159 -0.04217145 -0.28688719 -0.0616264  -0.10730528 -0.71960439\n",
      "  -0.81299299  0.27451636 -0.89091508 -1.15735526 -0.31229225 -0.15766702\n",
      "   2.2567235  -0.70470028  0.94326072]\n",
      " [ 0.74718833 -1.18894496  0.77325298 -1.18388064 -2.65917224  0.60631952\n",
      "  -1.75589058  0.45093446 -0.6840109   1.6595508   1.0685094  -0.4533858\n",
      "  -0.68783761 -1.2140774  -0.44092263]\n",
      " [-0.2803555  -0.36469354  0.15670386  0.5785215   0.34965446 -0.76414392\n",
      "  -1.43779147  1.36453185 -0.68944918 -0.6522936  -0.52118931 -1.84306955\n",
      "  -0.477974   -0.47965581  0.6203583 ]\n",
      " [ 0.69845715  0.00377089  0.93184837  0.33996498 -0.01568211  0.16092817\n",
      "  -0.19065349 -0.39484951 -0.26773354 -1.12801133  0.28044171 -0.99312361\n",
      "   0.84163126 -0.24945858  0.04949498]]\n",
      "Wy: [[ 0.49383678  0.64331447 -1.57062341 -0.20690368  0.88017891]\n",
      " [-1.69810582  0.38728048 -2.25556423 -1.02250684  0.03863055]\n",
      " [-1.6567151  -0.98551074 -1.47183501  1.64813493  0.16422776]\n",
      " [ 0.56729028 -0.2226751  -0.35343175 -1.61647419 -0.29183736]\n",
      " [-0.76149221  0.85792392  1.14110187  1.46657872  0.85255194]\n",
      " [-0.59865394 -1.11589699  0.76666318  0.35629282 -1.76853845]\n",
      " [ 0.35548179  0.81451982  0.05892559 -0.18505367 -0.80764849]\n",
      " [-1.4465347   0.80029795 -0.30911444 -0.23346666  1.73272119]\n",
      " [ 0.68450111  0.370825    0.14206181  1.51999486  1.71958931]\n",
      " [ 0.92950511  0.58222459 -2.09460307  0.12372191 -0.13010695]\n",
      " [ 0.09395323  0.94304609 -2.73967717 -0.56931205  0.26990435]\n",
      " [-0.46684555 -1.41690611  0.86896349  0.27687191 -0.97110457]\n",
      " [ 0.3148172   0.82158571  0.00529265  0.8005648   0.07826018]\n",
      " [-0.39522898 -1.15942052 -0.08593077  0.19429294  0.87583276]\n",
      " [-0.11510747  0.45741561 -0.96461201 -0.78262916 -0.1103893 ]]\n",
      "bh: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "by: [[0. 0. 0. 0. 0.]]\n",
      "(8, 15)\n",
      "[[-0.99999848  0.99990248 -0.99996607 -0.99964416 -0.99988767  0.99908206\n",
      "  -0.99245617  0.99774775  0.97661676 -0.99746223  0.99999904 -0.99058843\n",
      "  -0.99202901 -0.99926176 -0.99999667]\n",
      " [-0.99268074  0.99986974 -0.9999067   0.26496763 -0.99999992  0.99365559\n",
      "   0.99997865 -0.92923321  0.9999915   0.99999973 -0.99999416 -0.99999998\n",
      "   0.99883056  0.99975776 -0.93935595]\n",
      " [-0.36902575  0.44492003 -0.99944275 -0.99995563 -0.99992097  0.99665852\n",
      "   0.72379803 -0.99999326 -0.99999954  0.94773029 -0.97691994 -0.99977637\n",
      "   0.99980692 -0.67651382 -0.99156369]\n",
      " [-0.39806064 -0.99999418 -0.99310123 -1.         -1.         -0.98585334\n",
      "  -0.99999405 -0.86267795 -0.99999684  0.99762024  0.51839154 -0.99999769\n",
      "   0.83558747 -0.9998692   0.58947407]\n",
      " [-0.99993686  0.99998677  0.81137977 -0.99854303 -0.99556855  0.99953662\n",
      "  -0.85555078 -0.98745137  0.99413322 -0.85880888 -0.99999992 -0.99999995\n",
      "  -0.99997633  0.99973741 -0.99869053]\n",
      " [-0.9950876   0.99994904 -0.25654338 -0.99954077 -0.90971218 -0.99698643\n",
      "   0.89590124 -1.         -0.75081061 -0.99999017  0.96185436  0.99998106\n",
      "   1.         -0.99885591  0.99871836]\n",
      " [ 0.99900693  0.99999998  0.99868214  1.          0.99999998  0.95036811\n",
      "   0.98572661 -0.99999124  0.99999997  0.99999834 -0.99994008  0.99999994\n",
      "  -0.84676252  0.9999987  -0.95978065]\n",
      " [-0.99696688 -0.999886    0.04534836 -0.9992306  -0.9739127   1.\n",
      "  -0.99999982 -0.99999987 -0.99974037  0.55317951 -0.66867349  0.67942504\n",
      "   0.99999786 -0.99988625 -0.70956345]]\n",
      "(8, 5)\n",
      "[[1.50328186e-01 1.29400413e-01 6.14354644e-02 2.35274383e-03\n",
      "  6.56483193e-01]\n",
      " [9.94092370e-01 5.87047609e-04 4.90027791e-03 2.00413513e-04\n",
      "  2.19891436e-04]\n",
      " [9.85207589e-01 2.78196514e-03 1.18935976e-02 1.11375379e-04\n",
      "  5.47286326e-06]\n",
      " [9.97514909e-01 2.42656583e-03 1.15037301e-05 1.89191768e-06\n",
      "  4.51297575e-05]\n",
      " [3.54722882e-02 4.82841223e-05 7.08650891e-01 2.04258139e-01\n",
      "  5.15703974e-02]\n",
      " [7.82585179e-01 2.08891987e-01 6.72865883e-03 6.11072148e-04\n",
      "  1.18310327e-03]\n",
      " [4.50921405e-01 9.84190850e-04 2.73752410e-02 4.67680649e-01\n",
      "  5.30385145e-02]\n",
      " [3.37730695e-01 1.84532669e-05 6.57162397e-01 5.08562982e-03\n",
      "  2.82459439e-06]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "rnn_cell = RNNCell(10, 15, 5)\n",
    "print(\"Wh:\", rnn_cell.Wh)\n",
    "print(\"Wy:\", rnn_cell.Wy)\n",
    "print(\"bh:\", rnn_cell.bh)\n",
    "print(\"by:\", rnn_cell.by)\n",
    "rnn_cell.bh = np.random.randn(1, 15)\n",
    "rnn_cell.by = np.random.randn(1, 5)\n",
    "h_prev = np.random.randn(8, 15)\n",
    "x_t = np.random.randn(8, 10)\n",
    "h, y = rnn_cell.forward(h_prev, x_t)\n",
    "print(h.shape)\n",
    "print(h)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732372ea",
   "metadata": {},
   "source": [
    "## 1. RNN\n",
    "\n",
    "The `rnn` function performs **forward propagation for a simple Recurrent Neural Network (RNN) over multiple time steps** using an instance of `RNNCell`. It processes a sequence of inputs, updating hidden states and generating outputs for each time step.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "#### 1. Initialize Hidden States and Outputs\n",
    "- Set up arrays to store all hidden states and outputs\n",
    "\n",
    "#### 2. Iterate Over Time Steps\n",
    "- For each time step, update the hidden state and generate the output using the `RNNCell`\n",
    "\n",
    "#### 3. Return results\n",
    "- Return arrays containing all hidden states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2910d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"RNN Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rnn(rnn_cell, X, h_0):\n",
    "    \"\"\"\n",
    "        Performs forward propagation for a simple RNN:\n",
    "    \"\"\"\n",
    "\n",
    "    # Array to store all hidden states\n",
    "    H = np.zeros((X.shape[0] + 1, h_0.shape[0], h_0.shape[1]))        # include initial hidden state\n",
    "    H[0] = h_0                                                        # Set the first hidden state to h_0\n",
    "\n",
    "    # Array to store all outputs\n",
    "    Y = np.zeros((X.shape[0], X.shape[1], rnn_cell.by.shape[1]))      # to match the time steps, batch size, and output dimensionality \n",
    "\n",
    "    # Loop over each time step (i) and corresponding input (x_t)\n",
    "    for i, x_t in enumerate(X):\n",
    "        H[i + 1], Y[i] = rnn_cell.forward(H[i], x_t)\n",
    "\n",
    "    # Return all hidden states and all outputs\n",
    "    return H, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaa6e1",
   "metadata": {},
   "source": [
    "## 1. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a806523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 8, 15)\n",
      "[[[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.28045646  0.31045845  0.99989202  0.99376927 -0.79365613\n",
      "   -0.96748347  0.87706931  0.99999947  0.99994207  0.99846898\n",
      "   -0.9998936  -0.99599063 -0.98354763  0.91448074 -0.99979356]\n",
      "  [ 0.99999489  0.89822683  0.24147191 -0.9803711   0.97621493\n",
      "   -0.98164731 -0.99994247  0.8986866  -0.99999887 -0.99143954\n",
      "   -0.96021992  0.69187855  0.99995412 -0.99854789  0.96504703]\n",
      "  [-0.99374166  0.02430002  0.99860303  0.4806379  -0.95292346\n",
      "   -0.94778081  0.991254    0.99993389  0.99993849  0.99997438\n",
      "    0.2696434  -0.99997511 -0.85786199  0.99935457 -0.89784401]\n",
      "  [-0.99979182  0.02093496 -0.72250908  0.23679642  0.99706722\n",
      "   -0.73430469  0.48017722 -0.99801748 -0.99338301 -0.76416571\n",
      "    0.79033555  0.77277814  0.93892152  0.9979191   0.27925815]\n",
      "  [-0.08989532  0.72431641 -0.9834093  -0.97786125  0.99273298\n",
      "    0.0648553   0.84989032 -0.99598787  0.9715782  -0.99881274\n",
      "   -0.99824865 -0.927426   -0.90669173 -0.99667088  0.22362337]\n",
      "  [ 0.99999349  0.99907395 -0.7328029  -0.82304935 -0.99840166\n",
      "    0.39011906 -0.97274826  0.93085651  0.96715781  0.99934149\n",
      "   -0.89743926 -0.99989968  0.99999499 -0.9992797   0.99999925]\n",
      "  [ 0.99888157  0.94903062  0.6019996  -0.75694962  0.9791763\n",
      "   -0.99782011 -0.98474459  0.99223421 -0.13680496  0.99841024\n",
      "   -0.84903426 -0.99979527  0.9954165  -0.98588104  0.98710578]\n",
      "  [-0.94010273  0.97958914  0.98473426  0.99992105  0.9942734\n",
      "   -0.99992946 -0.96419606  0.87329534 -0.99969588 -0.84355236\n",
      "   -0.99990703 -0.15378654 -0.99639429  0.99933908 -0.99999999]]\n",
      "\n",
      " [[ 0.9815382   0.99975809 -0.99999611 -0.99889033  0.99999956\n",
      "   -0.73199948 -0.99997184 -0.99999878 -0.99999997  0.16158584\n",
      "   -0.12582106  0.57804071 -0.36649548 -0.96993281  0.99996427]\n",
      "  [ 1.          0.96687647 -0.9999892  -0.99999959 -0.98924235\n",
      "   -0.89048619 -0.81080926  0.99914494  0.9998444  -0.46987305\n",
      "    0.99999997 -0.8528737   0.99999703  0.97480799  0.99999998]\n",
      "  [ 0.42638934  0.99993147 -0.98858238  0.65298896 -0.89179354\n",
      "    0.93632617 -0.99999792 -0.97639015 -0.98939696  0.99984502\n",
      "   -0.99998672 -0.9923783  -0.99894376  0.99960725  0.99997181]\n",
      "  [ 0.98772319 -0.96799441  0.99999999 -0.27872584  0.85383884\n",
      "   -0.8958682   0.06974888  1.          0.9799729  -0.99885011\n",
      "   -0.99699069  0.4808139   0.99999974 -0.99968142 -0.97519996]\n",
      "  [ 0.9999942   0.9999832  -0.99083785 -0.99983663 -0.13661462\n",
      "    0.99681593 -0.99955258 -0.99878508 -0.99999632  0.9837929\n",
      "    0.95365885  0.71883932  0.99994997 -0.20605515  0.9860499 ]\n",
      "  [-0.9997258  -0.70631316 -0.99997618 -0.79925605  0.76156785\n",
      "   -0.99924321  0.99995866 -0.99999192 -0.99851465  0.99995641\n",
      "    0.99999921 -0.99825848  0.56082086  0.99972991  0.99214777]\n",
      "  [-0.97776349  0.98531597 -0.98603718 -0.99997529 -0.99877658\n",
      "   -0.99846643  0.99990333  0.99998432  0.99999995  0.97310843\n",
      "    0.99957781 -0.99999256 -0.9928038   0.99999749 -0.99971894]\n",
      "  [-0.58848448  0.99999996  0.94813681 -0.99985365  0.9987632\n",
      "   -0.99787217 -0.99999009  0.99999991 -0.70731186 -0.49599127\n",
      "   -0.99999292 -0.53067278  0.4024293  -0.99915486 -0.67705857]]\n",
      "\n",
      " [[ 0.02159932 -0.05766207 -0.94671164 -0.99952243  0.99023373\n",
      "   -0.99813696  0.98165231  0.95986812  0.73822398 -0.99979411\n",
      "    0.99986866  0.91883766  0.9319941   0.90178918 -0.99832848]\n",
      "  [ 0.99576182 -1.          0.86495505  0.98717356  0.14114621\n",
      "   -0.97843701  1.         -0.60102639  0.99999885 -0.57861717\n",
      "   -0.98868325 -0.99999997 -0.99999699 -0.69776196  0.99987619]\n",
      "  [-0.55280313  0.99976263  0.99988335 -0.999997    0.99237028\n",
      "   -0.99309361 -0.9999836   0.98585978  0.99983873  0.32184158\n",
      "    0.99234585 -0.99995741 -0.047329   -0.99984502  0.79266748]\n",
      "  [ 0.99980497  0.99998602 -0.99740112 -0.13760479  0.99965104\n",
      "   -0.99999817 -0.87675883  0.49148907 -1.          0.7377697\n",
      "    0.40436698  0.90998368  0.99989247  0.83571544 -0.95175215]\n",
      "  [-0.99999984  0.74499587  0.11261136  0.99998851  0.66476238\n",
      "   -0.99992827  0.77148665  0.84746372  0.72194754 -0.5670646\n",
      "    0.99999972 -0.94629     0.99999989  0.97452966  0.88199388]\n",
      "  [ 0.94859395  0.99983854  0.99197782  0.95895055  0.99796328\n",
      "   -0.97890206 -0.99998505  0.95152523 -0.17100548 -0.89926769\n",
      "   -0.99999915 -0.41555534  1.         -0.99957806 -0.99986577]\n",
      "  [ 0.83180133  0.89324935 -0.99944177  0.99187644  0.99998602\n",
      "   -0.99662278 -0.99993212 -0.64624145 -1.          0.35314965\n",
      "   -0.99999639 -0.39302021 -0.99986923 -0.56620403  0.78013651]\n",
      "  [-0.87272308  0.99999776 -0.99641196 -0.99995886  0.98727232\n",
      "   -0.99997324 -0.99998595  0.99999381 -0.91757624 -0.98401138\n",
      "   -0.99995904  0.98699872 -0.99999997  1.         -0.99999795]]\n",
      "\n",
      " [[ 0.99973886  0.99132092 -0.63798222  0.99999984  0.99763983\n",
      "   -0.99770045  0.92359832  0.99950424 -0.99999996 -0.99924437\n",
      "   -0.99987421  0.90362999  0.99997099 -0.99292097  0.99850743]\n",
      "  [ 0.92764281 -0.44729884  0.53546242 -0.99999879 -0.91734635\n",
      "    0.99976384 -0.05921623 -1.         -0.96364256  0.71184662\n",
      "    0.99999792  0.33194123  0.62158957  0.99999966 -0.99126856]\n",
      "  [-0.99984019  0.987753    0.99950054 -0.93855015 -0.97806637\n",
      "   -0.99103239 -0.99987339  1.          0.94280891  0.99654101\n",
      "    0.93116071 -0.99994871 -0.99995264  0.99999628 -0.99998721]\n",
      "  [-0.95371495  0.94934198 -0.96334859  0.56366756  0.99989258\n",
      "   -0.99999997  0.99999982  0.99995748  0.9996443  -0.99999822\n",
      "   -0.99996934 -0.999464   -0.99960469  0.59891462  0.99636704]\n",
      "  [-0.9996027  -0.30383752  0.99999901  0.92225063  0.84680583\n",
      "    0.85076494  0.9549862   0.99629471  0.99996193 -0.92100452\n",
      "   -0.99996395 -0.95870281 -0.31280611 -0.04465962 -0.93219184]\n",
      "  [ 0.99961341  0.99999987  0.86089999 -0.99980201  0.99996125\n",
      "   -1.         -0.99593006  0.99999861 -0.9996089   0.99998952\n",
      "   -0.99984119 -0.99995972 -0.73018449  0.99659812 -0.32810449]\n",
      "  [ 0.49499662  0.94878701  0.99998349 -0.99999992  0.93379934\n",
      "   -0.96794208  0.98152386  0.99999881  0.99985026 -0.98271635\n",
      "    0.98016441  0.66655832 -0.36553736 -0.88051456 -0.99999605]\n",
      "  [-0.98906341  1.         -0.9990707   0.56028665  0.99977379\n",
      "   -0.9999998   0.81034224  0.99999838 -0.96947611 -0.28188414\n",
      "   -1.         -0.72740659 -0.99997677 -0.99986566 -0.99949994]]\n",
      "\n",
      " [[-0.99938611  0.99999992  0.99813296 -0.99991354 -0.90570141\n",
      "   -0.99999959  0.99847095  0.99998858 -0.99972975  0.96599203\n",
      "    0.99989105  0.99998407  0.99992     0.99999936 -0.99999991]\n",
      "  [-0.71719667 -0.88874829 -0.99999958  0.82287298  0.99999962\n",
      "   -0.99962592 -0.90300796 -0.99999999  0.99233587 -0.99999978\n",
      "   -0.99960591 -0.99997517 -0.94404022 -0.95862619  0.99999817]\n",
      "  [-0.96449187 -0.74738847 -0.99966005  0.05593934  0.91594293\n",
      "   -0.77541434 -0.99818207  0.98403668 -0.99998974  0.99412781\n",
      "   -0.99991264 -0.99990515 -0.78348295 -0.99992008  0.99998808]\n",
      "  [ 0.9058035   0.96669291  0.98574722 -0.99999363  0.83068998\n",
      "    0.99665673  0.9839001  -0.71499509 -0.28285442 -0.9932421\n",
      "   -0.99993026  0.85980809 -0.99998982  0.93679885 -0.99631899]\n",
      "  [-0.74371926  0.99868899  0.02380641 -0.99990951  0.83917959\n",
      "    0.99982723 -0.98293373 -0.9953362   0.40475569 -0.46463964\n",
      "   -0.99997256 -0.22627056 -0.99771933  0.99995235  0.99975447]\n",
      "  [ 0.99956181 -0.57223021 -1.         -1.          0.99961364\n",
      "    0.20530549  0.99999082 -0.80890552  1.         -0.99926037\n",
      "   -0.99353919 -0.99999997 -0.99890894 -0.99993789  1.        ]\n",
      "  [ 1.         -0.88851657 -0.99936741 -0.93368475 -0.99858243\n",
      "   -0.8420648  -0.49278096  0.99999939 -0.9987008   0.66207558\n",
      "   -0.99940121 -0.98451163 -0.64710583  0.91651861  0.99999565]\n",
      "  [-0.92335828  0.9999984  -0.99921281 -0.99999848  0.99987883\n",
      "    0.14368028 -0.83952698  0.33506722 -0.99983511 -0.71454207\n",
      "   -0.99094859  0.99998169 -0.93897545  0.94914723 -0.99999569]]\n",
      "\n",
      " [[ 0.81091007  0.99999978 -0.99998584  0.99999927 -0.94066741\n",
      "   -0.99988382 -0.99999856  0.99999999  0.32992442  0.99999663\n",
      "   -0.99986172 -0.94084609  0.99996864 -0.93731369  0.9981379 ]\n",
      "  [ 0.99666395  0.95060637  0.99999853 -0.99991407 -0.33407483\n",
      "    0.9999924  -0.99848749 -0.99959001 -0.99942914 -0.99999891\n",
      "    0.47030998  0.99925872 -0.99454294  0.98317593 -0.9999999 ]\n",
      "  [ 0.99999389 -0.9999719   0.54181066 -0.99998821  0.70840716\n",
      "    0.99305167  0.99917166  0.99977397  0.9999967   0.53627041\n",
      "   -0.99883948  0.56148125  0.65566993  0.18684821 -0.99994887]\n",
      "  [-0.97778943  0.99999991 -0.37748688 -0.99852086 -0.96325587\n",
      "   -0.99993016  0.52183541  0.99999989  0.99865056  0.96606915\n",
      "   -0.99970049 -0.99999344 -0.99955457  0.91387983 -0.92541392]\n",
      "  [-0.33524436 -0.99228988  0.99053526 -0.9999978   0.56645869\n",
      "    0.98686703  0.97420129  0.99971929  0.99999994 -0.99885937\n",
      "   -0.99998253 -0.99966099 -0.99986669 -0.98748643  0.99519547]\n",
      "  [ 0.99999936  0.88242893  0.99972709 -0.99989659 -0.98646752\n",
      "    0.98350029  0.91751893  0.99807048  0.77214128  0.99575033\n",
      "    0.99511003 -0.99817847  0.93593858 -0.96661803 -0.99999965]\n",
      "  [ 0.99999871  0.98706162 -0.99962856 -0.99999997  0.99299962\n",
      "   -0.99991992 -0.37353737 -0.99861834  0.80033388 -0.999521\n",
      "   -0.99645465 -0.99995551 -0.99997417 -0.99960322 -0.99937268]\n",
      "  [-0.97563483  1.          0.86680386  0.9478412   0.85215271\n",
      "   -0.99999729 -0.99879042  1.          0.05365269 -0.98268965\n",
      "   -1.         -0.6138904  -0.99999967 -0.93869083 -0.99999815]]]\n",
      "(6, 8, 5)\n",
      "[[[6.02764349e-04 9.78573782e-01 1.46585015e-03 1.93515759e-02\n",
      "   6.02762159e-06]\n",
      "  [6.12269689e-01 6.29839290e-02 1.62654094e-01 7.93670905e-06\n",
      "   1.62084351e-01]\n",
      "  [1.99736920e-03 5.22377931e-02 4.67547169e-02 8.99000614e-01\n",
      "   9.50642515e-06]\n",
      "  [7.11110766e-02 3.27336483e-07 2.32458061e-03 9.56977764e-02\n",
      "   8.30866239e-01]\n",
      "  [1.41053218e-01 1.71447100e-05 2.34415896e-01 2.34153123e-01\n",
      "   3.90360618e-01]\n",
      "  [1.68918850e-05 9.85785166e-01 1.40916427e-02 1.76315123e-05\n",
      "   8.86681940e-05]\n",
      "  [2.18270168e-02 5.06723004e-01 1.28214688e-01 5.94005471e-06\n",
      "   3.43229351e-01]\n",
      "  [6.42820710e-01 1.14977041e-01 2.07116979e-02 2.21259809e-01\n",
      "   2.30741223e-04]]\n",
      "\n",
      " [[4.27688402e-02 1.28547418e-04 7.02097080e-05 2.87816217e-07\n",
      "   9.57032115e-01]\n",
      "  [8.77512203e-02 1.45722683e-01 7.30047202e-01 3.60173974e-02\n",
      "   4.61497335e-04]\n",
      "  [1.88771525e-02 7.18871246e-01 1.12523544e-04 6.46568826e-02\n",
      "   1.97482196e-01]\n",
      "  [4.02519423e-04 2.77138307e-02 9.71758006e-01 1.00608781e-04\n",
      "   2.50347609e-05]\n",
      "  [1.68081359e-03 1.19914660e-03 6.22096569e-05 7.52586383e-06\n",
      "   9.97050304e-01]\n",
      "  [1.65623211e-03 1.84309538e-09 7.04837061e-05 1.22898450e-03\n",
      "   9.97044298e-01]\n",
      "  [3.31019436e-02 2.19904501e-03 1.67712781e-02 9.47869654e-01\n",
      "   5.80793423e-05]\n",
      "  [3.88821566e-01 7.46006953e-02 5.26187330e-01 1.44664463e-03\n",
      "   8.94376411e-03]]\n",
      "\n",
      " [[1.27127879e-01 2.55178577e-05 7.11422168e-01 1.59518772e-01\n",
      "   1.90566283e-03]\n",
      "  [1.92396643e-02 5.66697620e-02 8.69674118e-01 2.62530433e-02\n",
      "   2.81634123e-02]\n",
      "  [1.95622455e-03 4.56324338e-04 9.97387793e-01 5.67342097e-05\n",
      "   1.42923666e-04]\n",
      "  [3.23852787e-01 2.73246507e-01 9.69547485e-04 2.74693676e-04\n",
      "   4.01656465e-01]\n",
      "  [2.63927720e-03 2.30597523e-05 2.89829584e-01 7.06800178e-01\n",
      "   7.07901653e-04]\n",
      "  [5.87307049e-04 9.94837879e-01 4.52722586e-03 1.80139545e-05\n",
      "   2.95742406e-05]\n",
      "  [8.48929070e-02 1.99892283e-01 4.75633094e-04 7.49325498e-05\n",
      "   7.14664244e-01]\n",
      "  [9.86862092e-01 3.15973762e-04 4.59252609e-03 8.07840715e-03\n",
      "   1.51000779e-04]]\n",
      "\n",
      " [[1.50251289e-01 1.13349048e-01 3.97280374e-02 2.10540508e-04\n",
      "   6.96461085e-01]\n",
      "  [1.20501119e-01 1.26276637e-01 2.32078711e-03 7.33183198e-02\n",
      "   6.77583136e-01]\n",
      "  [6.83492105e-02 5.12578287e-01 1.99424597e-01 2.19638835e-01\n",
      "   9.06990368e-06]\n",
      "  [1.61749503e-02 1.24393171e-05 2.33386499e-01 7.50178542e-01\n",
      "   2.47569293e-04]\n",
      "  [1.29383732e-05 1.39683586e-04 2.26281663e-01 7.73565165e-01\n",
      "   5.50060522e-07]\n",
      "  [8.44947562e-01 1.20867399e-01 7.94345461e-04 6.71232740e-05\n",
      "   3.33235706e-02]\n",
      "  [2.44169572e-01 2.20121690e-03 7.51974640e-01 1.15433455e-03\n",
      "   5.00236697e-04]\n",
      "  [5.54985544e-01 1.07253256e-01 9.29988727e-02 2.17693424e-01\n",
      "   2.70689034e-02]]\n",
      "\n",
      " [[8.88640162e-01 1.56698297e-02 5.87276466e-03 3.38939050e-02\n",
      "   5.59233387e-02]\n",
      "  [7.20440165e-04 1.51107080e-04 9.69401595e-01 2.89622444e-02\n",
      "   7.64613664e-04]\n",
      "  [5.18213410e-03 7.88054298e-02 9.01350047e-01 1.23750625e-03\n",
      "   1.34248832e-02]\n",
      "  [8.67188767e-01 5.99752369e-05 3.99155126e-04 2.91664829e-02\n",
      "   1.03185620e-01]\n",
      "  [1.56583601e-01 1.29704780e-05 6.58342234e-02 6.60285309e-01\n",
      "   1.17283896e-01]\n",
      "  [3.20428542e-02 2.90915538e-05 7.04098394e-01 2.05593966e-02\n",
      "   2.43270264e-01]\n",
      "  [1.10339169e-01 8.64722134e-01 1.87112048e-02 1.84963618e-03\n",
      "   4.37785621e-03]\n",
      "  [8.91331225e-01 3.16632800e-04 5.87351108e-03 9.84343760e-02\n",
      "   4.04425538e-03]]\n",
      "\n",
      " [[9.08880658e-07 9.99908252e-01 8.69753930e-05 4.21788139e-07\n",
      "   3.44145611e-06]\n",
      "  [9.75097062e-01 1.50206450e-02 2.12500754e-04 6.04149827e-03\n",
      "   3.62829443e-03]\n",
      "  [9.18773353e-04 1.90293262e-02 9.61409118e-01 1.48018825e-02\n",
      "   3.84089978e-03]\n",
      "  [6.39729040e-02 3.61861035e-02 3.21801012e-02 8.67488783e-01\n",
      "   1.72107765e-04]\n",
      "  [1.92204055e-05 2.30711346e-06 9.98321849e-01 1.65511248e-03\n",
      "   1.51079814e-06]\n",
      "  [1.94110352e-04 9.72377591e-01 2.60185598e-02 1.17648609e-03\n",
      "   2.33252810e-04]\n",
      "  [8.89470463e-01 5.79353700e-03 2.44302401e-02 6.32135155e-03\n",
      "   7.39844085e-02]\n",
      "  [1.46229725e-02 6.28168571e-01 3.38577126e-01 1.86234640e-02\n",
      "   7.86667391e-06]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "rnn_cell = RNNCell(10, 15, 5)\n",
    "rnn_cell.bh = np.random.randn(1, 15)\n",
    "rnn_cell.by = np.random.randn(1, 5)\n",
    "X = np.random.randn(6, 8, 10)\n",
    "h_0 = np.zeros((8, 15))\n",
    "H, Y = rnn(rnn_cell, X, h_0)\n",
    "print(H.shape)\n",
    "print(H)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586b598",
   "metadata": {},
   "source": [
    "## 2. GRU Cell\n",
    "\n",
    "<img src=\"figs/gru.png\" alt=\"logo\" width=\"400\"/>\n",
    "\n",
    "The `GRUCell` class represents a **single cell in a Gated Recurrent Unit (GRU) network**. A GRU is an advanced type of RNN that uses gating mechanisms (update and reset gates) to better capture dependencies in the data, mitigating issues like the vanishing gradient problem.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "#### Initialization:\n",
    "- Initialize weights and biases for update gate, reset gate, intermediate hidden state, and output.\n",
    "\n",
    "#### Forward Propagation:\n",
    "- Compute the reset gate (`r_t`).\n",
    "- Compute the update gate (`z_t`).\n",
    "- Compute the candidate hidden state (`h_tilde`).\n",
    "- Compute the next hidden state (`h_next`).\n",
    "- Compute the output (`output_t`).\n",
    "\n",
    "### Step by Step Explanation\n",
    "\n",
    "Each gate in the GRU cell serves a specific purpose, controlling the flow of information through the network. The **reset gate determines how much of the previous hidden state should be forgotten, the update gate controls the balance between retaining the old hidden state and incorporating the new candidate hidden state, and the next hidden state is a combination of these elements**. The output is then generated based on the new hidden state, typically using a softmax function for classification tasks.\n",
    "\n",
    "#### Compute Reset Gate\n",
    "\n",
    "$$\n",
    " r_t = \\frac{1}{1 + e^{\\left(-(h_x \\cdot W_r + b_r)\\right)}} \n",
    "$$\n",
    "\n",
    "##### Why\n",
    "\n",
    "- **Purpose**: The reset gate (`r_t`) controls how much of the previous hidden state (`h_prev`) should be forgotten. It helps the network decide whether to ignore parts of the hidden state that are not relevant to predicting the next state.\n",
    "- **Mechanism**: If `r_t` is close to `0`, the reset gate effectively \"resets\" the hidden state, forcing the network to focus on the new input (`x_t`). If `r_t` is close to `1`, it allows the network to keep the information from the previous hidden state.\n",
    "\n",
    "##### Why\n",
    "\n",
    "- **Concatenation**: Combine the previous hidden state (`h_prev`) and the current input (`x_t`) into a single vector (`h_x`).\n",
    "- **Linear Transformation**: Apply a linear transformation using the reset gate weights (`Wr`) and bias (`br`). This transformation projects the concatenated input-hidden state vector into the appropriate dimensionality for the reset gate.\n",
    "- **Sigmoid Activation**: The sigmoid function ensures that the output values of `r_t` are between `0` and `1`, which is necessary for the gate mechanism. \n",
    "\n",
    "#### Compute Update Gate\n",
    "\n",
    "$$\n",
    "z_t = \\frac{1}{1 + e^{\\left(-(h_x \\cdot W_z + b_z)\\right)}}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Why\n",
    "- **Purpose**: The update gate (`z_t`) determines how much of the new hidden state ($h_{\\text{tilde}}$) will be used to update the current hidden state ($h_{\\text{next}}$). \n",
    "- It decides the extent to which the network should retain the existing information versus incorporating the new information.\n",
    "\n",
    "- **Mechanism**: If `z_t` is close to `1`, the network updates most of the hidden state with the new candidate hidden state ($h_{\\text{tilde}}$). If `z_t` is close to `0`, the network retains most of the previous hidden state.\n",
    "\n",
    "##### How\n",
    "- **Concatenation**: Combine `h_prev` and `x_t` into `h_x`.\n",
    "- **Linear Transformation**: Apply a linear transformation using the update gate weights (`Wz`) and bias (`bz`). This transformation projects the concatenated input-hidden state vector into the appropriate dimensionality for the update gate.\n",
    "- **Sigmoid Activation**: The sigmoid function ensures that the output values of `z_t` are between `0` and `1`, which is necessary for the gate mechanism.\n",
    "\n",
    "\n",
    "#### Compute Candidate Hidden State\n",
    "\n",
    "$$\n",
    "rh_x = \\text{concatenate}((r_t * h_{\\text{prev}}, x_t), \\text{axis}=1)\n",
    "$$\n",
    "\n",
    "and also\n",
    "\n",
    "$$\n",
    "h_{\\text{tilde}} = \\tanh(rh_x \\cdot W_h + b_h)\n",
    "$$\n",
    "\n",
    "\n",
    "##### Why\n",
    "\n",
    "- **Purpose**: The candidate hidden state ($h_{\\text{tilde}}$) represents the potential new hidden state, taking into account the reset gate's influence. It incorporates the new input and the reset-modified previous hidden state.\n",
    "\n",
    "- **Mechanism**: The reset gate determines which parts of the previous hidden state should be considered (or ignored) when computing the candidate hidden state.\n",
    "\n",
    "\n",
    "##### How\n",
    "- **Reset Gate Application**: Modify the previous hidden state ($h_prev$) by element-wise multiplying it with the reset gate ($r_t$). This results in $r_t \\cdot h_{\\text{prev}}$, which selectively forgets parts of the hidden state.\n",
    "\n",
    "- **Concatenation**: Combine $r_t * h_{\\text{prev}}$ and $x_t$ into a single vector ($rh_x$).\n",
    "- **Linear Transformation**: Apply a linear transformation using the weights for the candidate hidden state ($Wh$) and bias ($bh$). This transformation projects the concatenated reset-modified hidden state and input vector into the appropriate dimensionality for the candidate hidden state.\n",
    "- **Tanh Activation**: The tanh activation function is used to ensure that the values of $h_{\\text{tilde}}$ are between `-1` and `1`, providing a bounded and smooth non-linearity. \n",
    "\n",
    "\n",
    "#### Compute Next Hidden State\n",
    "\n",
    "$$\n",
    "h_{\\text{next}} = (1 - z_t) * h_{\\text{prev}} + z_t * h_{\\text{tilde}}\n",
    "$$\n",
    "\n",
    "##### Why\n",
    "- **Purpose**: The next hidden state ($h_{\\text{next}}$) combines the previous hidden state ($h_{\\text{prev}}$) and the candidate hidden state ($h_{\\text{tilde}}$) based on the update gate ($z_t$). This helps the network decide how much of the new information should be mixed with the existing information.\n",
    "- **Mechanism**: The update gate determines the weight of the contribution from the candidate hidden state versus the previous hidden state.\n",
    "\n",
    "##### How\n",
    "Update Gate Application: Use the update gate ($z_t$) to create a weighted combination of $h_{\\text{prev}} and h_{\\text{tilde}}$.\n",
    "\n",
    "##### Linear Combination\n",
    "\n",
    " Retains part of the previous hidden state.\n",
    "$$\n",
    "(1 - z_t) * h_{\\text{prev}}\n",
    "$$\n",
    "and incorporates part of the candidate hidden state.\n",
    "$$\n",
    "z_t * h_{\\text{tilde}}\n",
    "$$\n",
    "\n",
    "#### Compute Output\n",
    "\n",
    "$$\n",
    "\\text{output}_t = \\text{softmax}(h_{\\text{next}} \\cdot W_y + b_y)\n",
    "$$\n",
    "\n",
    "\n",
    "##### Why\n",
    "\n",
    "- **Purpose**: The output (`output_t`) is the final output of the GRU cell for the current time step. It uses the next hidden state $(h_{\\text{next}})$ to generate a meaningful representation for the output.\n",
    "- **Mechanism**: The softmax function is typically used for multi-class classification tasks to convert the logits into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"GRU Cell Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GRUCell:\n",
    "    \"\"\"\n",
    "        Represents a gated recurrent unit:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "        \"\"\"\n",
    "            Weight and Bias Initialization\n",
    "            \n",
    "            Weights for update gate, reset gate, and intermediate hidden state respectively (Wz, Wr, Wh)\n",
    "            Weights for output (Wy)\n",
    "            \n",
    "        \"\"\"\n",
    "        self.Wz = np.random.normal(size=(i+h, h))\n",
    "        self.Wr = np.random.normal(size=(i+h, h))\n",
    "        self.Wh = np.random.normal(size=(i+h, h))\n",
    "        self.Wy = np.random.normal(size=(h, o))\n",
    "        \n",
    "        \"\"\"\n",
    "            Biases for update gate, reset gate, and intermediate hidden state\n",
    "            Bias for output\n",
    "        \"\"\"\n",
    "        self.bz = np.zeros((1, h))\n",
    "        self.br = np.zeros((1, h))\n",
    "        self.bh = np.zeros((1, h))\n",
    "        self.by = np.zeros((1, o))\n",
    "\n",
    "    def forward(self, h_prev, x_t):\n",
    "        \"\"\"\n",
    "            Performs forward propagation for one time step\n",
    "        \"\"\"\n",
    "\n",
    "        def softmax(x):\n",
    "            \"\"\"Compute softmax activation function\"\"\"\n",
    "            return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "        # Concatenate Input and Hidden State\n",
    "        h_x = np.concatenate((h_prev, x_t), axis=1)\n",
    "\n",
    "        # Compute Reset Gate\n",
    "        r_t = 1 / (1 + np.exp(-(np.dot(h_x, self.Wr) + self.br)))\n",
    "\n",
    "        # Compute Update Gate \n",
    "        z_t = 1 / (1 + np.exp(-(np.dot(h_x, self.Wz) + self.bz)))\n",
    "\n",
    "        # Compute Candidate Hidden State\n",
    "        rh_x = np.concatenate((r_t * h_prev, x_t), axis=1)     # Combine reset-modified hidden state and input\n",
    "        h_tilde = np.tanh(np.dot(rh_x, self.Wh) + self.bh)     # Apply the tanh activation to compute the candidate hidden state\n",
    "        \n",
    "        # Compute Next Hidden State\n",
    "        h_next = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "\n",
    "        # Compute Output\n",
    "        output_t = softmax(np.dot(h_next, self.Wy) + self.by)\n",
    "\n",
    "        return h_next, output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57570b00",
   "metadata": {},
   "source": [
    "## 2. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "gru_cell = GRUCell(10, 15, 5)\n",
    "print(\"Wz:\", gru_cell.Wz)\n",
    "print(\"Wr:\", gru_cell.Wr)\n",
    "print(\"Wh:\", gru_cell.Wh)\n",
    "print(\"Wy:\", gru_cell.Wy)\n",
    "print(\"bz:\", gru_cell.bz)\n",
    "print(\"br:\", gru_cell.br)\n",
    "print(\"bh:\", gru_cell.bh)\n",
    "print(\"by:\", gru_cell.by)\n",
    "gru_cell.bz = np.random.randn(1, 15)\n",
    "gru_cell.br = np.random.randn(1, 15)\n",
    "gru_cell.bh = np.random.randn(1, 15)\n",
    "gru_cell.by = np.random.randn(1, 5)\n",
    "h_prev = np.random.randn(8, 15)\n",
    "x_t = np.random.randn(8, 10)\n",
    "h, y = gru_cell.forward(h_prev, x_t)\n",
    "print(h.shape)\n",
    "print(h)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717fbad",
   "metadata": {},
   "source": [
    "## 3. LSTM Cell\n",
    "\n",
    "LSTM cell uses various gates (forget gate, input gate, output gate) and memory cells (cell state) to control the flow of information and selectively retain or discard information from previous time steps. This allows it to capture long-term dependencies and make predictions based on the current input and the past context.\n",
    "\n",
    "<img src=\"figs/lstm.png\" alt=\"logo\" width=\"400\"/>\n",
    "\n",
    "Here are the key steps involved in implementing an LSTM cell:\n",
    "\n",
    "\n",
    "1. **Input Transformation**: Concatenate the previous hidden state `h_prev` and the current input `x_t` to form an input vector `h_x`.\n",
    "\n",
    "\n",
    "2. **Forget Gate**: Compute the activation of the forget gate `ft` by applying the sigmoid function to a linear transformation of `h_x` using weights `Wf` and biases `bf`.\n",
    "\n",
    "\n",
    "3. **Input/Update Gate**: Compute the activation of the input/update gate it by applying the sigmoid function to a linear transformation of `h_x` using weights `Wu` and biases `bu`.\n",
    "\n",
    "\n",
    "4. **Candidate Value**: Compute the candidate value `cct` by applying the hyperbolic tangent (tanh) function to a linear transformation of `h_x` using weights `Wc` and biases `bc`.\n",
    "\n",
    "\n",
    "5. **Cell State Update**: Update the cell state `c_next` by combining the previous cell state `c_prev` with the candidate value cct and the forget and input gate activations. Multiply `c_prev` element-wise with ft and add the element-wise multiplication of `it` and `cct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbafbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"LSTM cell for a RRN\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTMCell:\n",
    "    \"\"\" LSTMCell unit \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "        \"\"\"\n",
    "        Initializer constructor\n",
    "        Args:\n",
    "            i: the dimensionality of the data\n",
    "            h: the dimensionality of the hidden state\n",
    "            o: he dimensionality of the outputs\n",
    "        \"\"\"\n",
    "\n",
    "        # weight for the cell\n",
    "        self.Wf = np.random.normal(size=(i + h, h))\n",
    "        self.Wu = np.random.normal(size=(i + h, h))\n",
    "        self.Wc = np.random.normal(size=(i + h, h))\n",
    "        self.Wo = np.random.normal(size=(i + h, h))\n",
    "        self.Wy = np.random.normal(size=(h, o))\n",
    "\n",
    "        # Bias of the cell\n",
    "        self.bf = np.zeros((1, h))\n",
    "        self.bu = np.zeros((1, h))\n",
    "        self.bc = np.zeros((1, h))\n",
    "        self.bo = np.zeros((1, h))\n",
    "        self.by = np.zeros((1, o))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"softmax function\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, h_prev, c_prev, x_t):\n",
    "        \"\"\"\n",
    "        forward propagation for one time step in a LSTM\n",
    "        Args:\n",
    "            h_prev: numpy.ndarray of shape (m, h) containing the previous\n",
    "                    m: hidden state\n",
    "            x_t: numpy.ndarray of shape (m, i) that contains the data\n",
    "                 input for the cell\n",
    "        Returns: h_next, y\n",
    "                 h_next: the next hidden state\n",
    "                 y: the output of the cell\n",
    "        \"\"\"\n",
    "        # previous hidden cell state\n",
    "        h_x = np.concatenate((h_prev.T, x_t.T), axis=0)\n",
    "\n",
    "        # forget gate activation vector\n",
    "        ft = self.sigmoid((h_x.T @ self.Wf) + self.bf)\n",
    "\n",
    "        # input/update gate activation vector\n",
    "        it = self.sigmoid((h_x.T @ self.Wu) + self.bu)\n",
    "\n",
    "        # candidate value\n",
    "        cct = np.tanh((h_x.T @ self.Wc) + self.bc)\n",
    "        c_next = ft * c_prev + it * cct\n",
    "\n",
    "        # output gate\n",
    "        ot = self.sigmoid((h_x.T @ self.Wo) + self.bo)\n",
    "\n",
    "        # compute hidden state\n",
    "        h_next = ot * np.tanh(c_next)\n",
    "\n",
    "        # final output of the cell\n",
    "        y = self.softmax((h_next @ self.Wy) + self.by)\n",
    "\n",
    "        return h_next, c_next, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab43c3",
   "metadata": {},
   "source": [
    "## 3. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2183d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "lstm_cell = LSTMCell(10, 15, 5)\n",
    "print(\"Wf:\", lstm_cell.Wf)\n",
    "print(\"Wu:\", lstm_cell.Wu)\n",
    "print(\"Wc:\", lstm_cell.Wc)\n",
    "print(\"Wo:\", lstm_cell.Wo)\n",
    "print(\"Wy:\", lstm_cell.Wy)\n",
    "print(\"bf:\", lstm_cell.bf)\n",
    "print(\"bu:\", lstm_cell.bu)\n",
    "print(\"bc:\", lstm_cell.bc)\n",
    "print(\"bo:\", lstm_cell.bo)\n",
    "print(\"by:\", lstm_cell.by)\n",
    "lstm_cell.bf = np.random.randn(1, 15)\n",
    "lstm_cell.bu = np.random.randn(1, 15)\n",
    "lstm_cell.bc = np.random.randn(1, 15)\n",
    "lstm_cell.bo = np.random.randn(1, 15)\n",
    "lstm_cell.by = np.random.randn(1, 5)\n",
    "h_prev = np.random.randn(8, 15)\n",
    "c_prev = np.random.randn(8, 15)\n",
    "x_t = np.random.randn(8, 10)\n",
    "h, c, y = lstm_cell.forward(h_prev, c_prev, x_t)\n",
    "print(h.shape)\n",
    "print(h)\n",
    "print(c.shape)\n",
    "print(c)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd3244",
   "metadata": {},
   "source": [
    "## 4. Deep RNN\n",
    "\n",
    "The `deep_rnn` function performs forward propagation for a deep RNN (Recurrent Neural Network) composed of **multiple layers of `RNNCell`** instances.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "- `H`: Array to store all hidden states across layers and time steps, initialized with zeros.\n",
    "- `Y`: Array to store all outputs across time steps, initialized with zeros.\n",
    "\n",
    "### Forward propagation loop\n",
    "\n",
    "- Iterate over each time step and each layer:\n",
    "    - `for i, x_t in enumerate(X)`: Loop over each time step `i` and corresponding input `x_t`.\n",
    "    - `for l, rnn_cell in enumerate(rnn_cells)`: Loop over each layer `l` and its corresponding RNNCell.\n",
    "        - Perform forward propagation using the current hidden state `H[i, l]` and input `x_t` for the current layer.\n",
    "        - Update `H[i + 1, l]` with the new hidden state computed by `rnn_cell.forward`.\n",
    "        - Update `Y[i]` with the output `Y` computed by `rnn_cell.forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62923f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Deep RNN Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def deep_rnn(rnn_cells, X, h_0):\n",
    "    \"\"\" Performs forward propagation for a simple RNN \"\"\"\n",
    "\n",
    "    H = np.zeros((X.shape[0] + 1, h_0.shape[0],\n",
    "                  h_0.shape[1], h_0.shape[2]))\n",
    "    H[0] = h_0\n",
    "\n",
    "    Y = np.zeros((X.shape[0], X.shape[1], rnn_cells[-1].by.shape[1]))\n",
    "\n",
    "    for i, x_t in enumerate(X):\n",
    "        for l, rnn_cell in enumerate(rnn_cells):\n",
    "            H[i + 1, l], Y[i] = rnn_cell.forward(H[i, l], x_t)\n",
    "            x_t = H[i + 1, l]\n",
    "\n",
    "    return H, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb859da",
   "metadata": {},
   "source": [
    "## 4 - Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f16f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "cell1 = RNNCell(10, 15, 1)\n",
    "cell2 = RNNCell(15, 15, 1)\n",
    "cell3 = RNNCell(15, 15, 5)\n",
    "rnn_cells = [cell1, cell2, cell3]\n",
    "for rnn_cell in rnn_cells:\n",
    "    rnn_cell.bh = np.random.randn(1, 15)\n",
    "cell3.by = np.random.randn(1, 5)\n",
    "X = np.random.randn(6, 8, 10)\n",
    "H_0 = np.zeros((3, 8, 15))\n",
    "H, Y = deep_rnn(rnn_cells, X, H_0)\n",
    "print(H.shape)\n",
    "print(H)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203ebed",
   "metadata": {},
   "source": [
    "## 5. Bidirectional Cell Forward\n",
    "\n",
    "### Forward and Backward Processing:\n",
    "\n",
    "A bidirectional RNN consists of two RNNs:\n",
    "- Forward RNN: Processes the sequence from start to end.\n",
    "- Backward RNN: Processes the sequence from end to start.\n",
    "- Each RNN computes hidden states based on the information it sees in its respective direction.\n",
    "\n",
    "### Concatenated Outputs:\n",
    "\n",
    "- At each time step, the hidden state of the bidirectional RNN is the concatenation of the hidden states from both the forward and backward RNNs.\n",
    "- This concatenated hidden state encodes information from both directions of the sequence.\n",
    "\n",
    "<img src=\"figs/birnn.png\" alt=\"logo\" width=\"700\"/>\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Contextual Understanding**: captures dependencies in both past and future contexts, improving the understanding of each sequence element.\n",
    "\n",
    "- **Improved Prediction Accuracy**: utilizes information from both directions, leading to more accurate predictions, especially in tasks requiring comprehensive context.\n",
    "\n",
    "- **Rich Feature Representation**: concatenates hidden states to provide richer feature representations, improving learning and extraction of nuanced sequential patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Bidirectional Cell Forward Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BidirectionalCell:\n",
    "    \"\"\"\n",
    "        Represents a bidirectional cell of a RNN:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "\n",
    "        # Initialize weights for forward and backward RNNs\n",
    "        self.Whf = np.random.normal(size=(i+h, h))      #  Weight matrix for forward RNN\n",
    "        self.Whb = np.random.normal(size=(i+h, h))      #  Weight matrix for backward RNN\n",
    "        self.Wy = np.random.normal(size=(h*2, o))       # Weight matrix for output layer (concatenated hidden states)\n",
    "\n",
    "        # Initialize biases for forward and backward RNNs\n",
    "        self.bhf = np.zeros((1, h))                     # forward RNN\n",
    "        self.bhb = np.zeros((1, h))                     # backward RNN\n",
    "        self.by = np.zeros((1, o))                      # output\n",
    "\n",
    "    def forward(self, h_prev, x_t):\n",
    "        \"\"\" Performs forward propagation for one time step \"\"\"\n",
    "\n",
    "        # Concatenate previous hidden state and current input\n",
    "        h_x = np.concatenate((h_prev, x_t), axis=1)\n",
    "\n",
    "        # Compute next hidden state using tanh activation function\n",
    "        h_next = np.tanh(np.dot(h_x, self.Whf) + self.bhf)\n",
    "\n",
    "        return h_next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77e018",
   "metadata": {},
   "source": [
    "## 5. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "bi_cell =  BidirectionalCell(10, 15, 5)\n",
    "print(\"Whf:\", bi_cell.Whf)\n",
    "print(\"Whb:\", bi_cell.Whb)\n",
    "print(\"Wy:\", bi_cell.Wy)\n",
    "print(\"bhf:\", bi_cell.bhf)\n",
    "print(\"bhb:\", bi_cell.bhb)\n",
    "print(\"by:\", bi_cell.by)\n",
    "bi_cell.bhf = np.random.randn(1, 15)\n",
    "h_prev = np.random.randn(8, 15)\n",
    "x_t = np.random.randn(8, 10)\n",
    "h = bi_cell.forward(h_prev, x_t)\n",
    "print(h.shape)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b7896",
   "metadata": {},
   "source": [
    "## 6. Bidirectional Cell Backward\n",
    "\n",
    "The backward method in the BidirectionalCell class **computes the previous hidden state for the backward RNN**. \n",
    "\n",
    "It **concatenates the current input and the next hidden state**, then **applies a tanh activation function** to the weighted sum of this concatenation and biases. \n",
    "\n",
    "This process **generates the previous hidden state (`h_prev`)**, essential for capturing dependencies in the reverse direction of sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b378fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Bidirectional Cell Forward Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BidirectionalCell:\n",
    "    \"\"\"\n",
    "        Represents a bidirectional cell of a RNN:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "\n",
    "        # Initialize weights for forward and backward RNNs\n",
    "        self.Whf = np.random.normal(size=(i+h, h))      #  Weight matrix for forward RNN\n",
    "        self.Whb = np.random.normal(size=(i+h, h))      #  Weight matrix for backward RNN\n",
    "        self.Wy = np.random.normal(size=(h*2, o))       # Weight matrix for output layer (concatenated hidden states)\n",
    "\n",
    "        # Initialize biases for forward and backward RNNs\n",
    "        self.bhf = np.zeros((1, h))                     # forward RNN\n",
    "        self.bhb = np.zeros((1, h))                     # backward RNN\n",
    "        self.by = np.zeros((1, o))                      # output\n",
    "\n",
    "    def forward(self, h_prev, x_t):\n",
    "        \"\"\" Performs forward propagation for one time step \"\"\"\n",
    "\n",
    "        # Concatenate previous hidden state and current input\n",
    "        h_x = np.concatenate((h_prev, x_t), axis=1)\n",
    "\n",
    "        # Compute next hidden state using tanh activation function\n",
    "        h_next = np.tanh(np.dot(h_x, self.Whf) + self.bhf)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "\n",
    "    def backward(self, h_next, x_t):\n",
    "        \"\"\"Calculates the hidden state in the backward direction \"\"\"\n",
    "\n",
    "        # Concatenate next hidden state and current input\n",
    "        h_x = np.concatenate((h_next, x_t), axis=1)\n",
    "\n",
    "        # Compute previous hidden state using tanh activation function\n",
    "        h_prev = np.tanh(np.dot(h_x, self.Whb) + self.bhb)\n",
    "\n",
    "        return h_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45332b3b",
   "metadata": {},
   "source": [
    "## 6 - Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ed243",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "bi_cell =  BidirectionalCell(10, 15, 5)\n",
    "bi_cell.bhb = np.random.randn(1, 15)\n",
    "h_next = np.random.randn(8, 15)\n",
    "x_t = np.random.randn(8, 10)\n",
    "h = bi_cell.backward(h_next, x_t)\n",
    "print(h.shape)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573c151",
   "metadata": {},
   "source": [
    "## 7. Bidirectional Output\n",
    "\n",
    "The goal is to calculate predictions for a bidirectional RNN across multiple time steps and layers. \n",
    "\n",
    "The method initializes an output matrix `Y` to store probabilities for each time step and layer, based on the hidden states `H`. \n",
    "\n",
    "For each time step, it computes raw outputs by applying weights Wy to each hidden state h and adding biases by. These raw outputs are then converted into probabilities using the softmax function, ensuring they represent the likelihood of each class for the given input sequence. This method effectively transforms sequential hidden states into actionable predictions, essential for tasks like sequence classification or prediction in natural language processing and time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Bidirectional Cell Forward Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BidirectionalCell:\n",
    "    \"\"\"\n",
    "        Represents a bidirectional cell of a RNN:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i, h, o):\n",
    "\n",
    "        # Initialize weights for forward and backward RNNs\n",
    "        self.Whf = np.random.normal(size=(i+h, h))      #  Weight matrix for forward RNN\n",
    "        self.Whb = np.random.normal(size=(i+h, h))      #  Weight matrix for backward RNN\n",
    "        self.Wy = np.random.normal(size=(h*2, o))       # Weight matrix for output layer (concatenated hidden states)\n",
    "\n",
    "        # Initialize biases for forward and backward RNNs\n",
    "        self.bhf = np.zeros((1, h))                     # forward RNN\n",
    "        self.bhb = np.zeros((1, h))                     # backward RNN\n",
    "        self.by = np.zeros((1, o))                      # output\n",
    "\n",
    "    def forward(self, h_prev, x_t):\n",
    "        \"\"\" Performs forward propagation for one time step \"\"\"\n",
    "\n",
    "        # Concatenate previous hidden state and current input\n",
    "        h_x = np.concatenate((h_prev, x_t), axis=1)\n",
    "\n",
    "        # Compute next hidden state using tanh activation function\n",
    "        h_next = np.tanh(np.dot(h_x, self.Whf) + self.bhf)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "\n",
    "    def backward(self, h_next, x_t):\n",
    "        \"\"\"Calculates the hidden state in the backward direction \"\"\"\n",
    "\n",
    "        # Concatenate next hidden state and current input\n",
    "        h_x = np.concatenate((h_next, x_t), axis=1)\n",
    "\n",
    "        # Compute previous hidden state using tanh activation function\n",
    "        h_prev = np.tanh(np.dot(h_x, self.Whb) + self.bhb)\n",
    "\n",
    "        return h_prev\n",
    "\n",
    "    def output(self, H):\n",
    "        \"\"\"Calculates all outputs for the RNN \"\"\"\n",
    "        \n",
    "        # Initialize Y with zeros, where H.shape[0] is the number of time steps, H.shape[1] is the number of layers,\n",
    "        # and self.Wy.shape[1] is the output dimensionality.\n",
    "        Y = np.zeros((H.shape[0], H.shape[1], self.Wy.shape[1]))\n",
    "\n",
    "        # Iterate over each time step i and corresponding hidden state h in H.\n",
    "        for i, h in enumerate(H):\n",
    "            # Calculate the raw output scores for each layer using dot product of h and weight matrix Wy, and add bias by.\n",
    "            outputs = np.dot(h, self.Wy) + self.by\n",
    "            \n",
    "            \"\"\"\n",
    "                  Compute the softmax function to convert raw outputs into probabilities.\n",
    "                  np.exp(outputs) calculates the exponentials of the outputs,\n",
    "                  np.sum(np.exp(outputs), axis=1, keepdims=True) sums the exponentials across the second axis (classes),\n",
    "                  and keeps the resulting array in the same shape as np.exp(outputs).\n",
    "            \"\"\"\n",
    "            Y[i] = np.exp(outputs) / np.sum(np.exp(outputs), axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        return Y   # calculated probabilities for each time step and layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7306ee",
   "metadata": {},
   "source": [
    "## 7 - Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "bi_cell =  BidirectionalCell(10, 15, 5)\n",
    "bi_cell.by = np.random.randn(1, 5)\n",
    "H = np.random.randn(6, 8, 30)\n",
    "Y = bi_cell.output(H)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e3c89",
   "metadata": {},
   "source": [
    "## 8. Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Bidirectional RNN Module\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bi_rnn(bi_cell, X, h_0, h_t):\n",
    "    \"\"\"Performs forward propagation for a simple RNN \"\"\"\n",
    "\n",
    "    Hf = np.zeros((X.shape[0], h_0.shape[0], h_0.shape[1]))\n",
    "    Hb = np.zeros(Hf.shape)\n",
    "    Hf[0] = bi_cell.forward(h_0, X[0])\n",
    "    Hb[-1] = bi_cell.backward(h_t, X[-1])\n",
    "\n",
    "    for i in range(1, len(X)):\n",
    "        x_tf = X[i]\n",
    "        x_tb = X[-(i + 1)]\n",
    "\n",
    "        Hf[i] = bi_cell.forward(Hf[i - 1], x_tf)\n",
    "        Hb[-(i + 1)] = bi_cell.backward(Hb[-i], x_tb)\n",
    "\n",
    "    H = np.concatenate((Hf, Hb), axis=-1)\n",
    "\n",
    "    return H, bi_cell.output(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed962017",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "bi_cell =  BidirectionalCell(10, 15, 5)\n",
    "X = np.random.randn(6, 8, 10)\n",
    "h_0 = np.zeros((8, 15))\n",
    "h_T = np.zeros((8, 15))\n",
    "H, Y = bi_rnn(bi_cell, X, h_0, h_T)\n",
    "print(H.shape)\n",
    "print(H)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf69f9b",
   "metadata": {},
   "source": [
    "### Happy Coding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
